import requests

def get(url):
    #print(url)
    x = ''
    while not x:
        try:
            x = requests.get(url)
        except:
            print('get failed')
    return x.text

def get_chunks(url):
    lst = []
    #print(url)
    try:
        # NOTE the stream=True parameter below
        with requests.get(url, stream=True) as r:
            r.raise_for_status()
            for chunk in r.iter_content(chunk_size=8192): 
                # If you have chunk encoded response uncomment if
                # and set chunk_size parameter to None.
                #if chunk: 
                lst.append(chunk.decode())
    except:
        print('get failed')
    #print(lst[0][:10], len(lst))
    return ''.join(lst)

import time
#from bs4 import BeautifulSoup
from selenium import webdriver
from webdriver_manager.chrome import ChromeDriverManager

def getScroll(url):
    driver = webdriver.Chrome(ChromeDriverManager().install())
    driver.implicitly_wait(30)

    try:
        SCROLL_PAUSE_TIME = 0.6
        driver.get(f'{url}')

        last_height = driver.execute_script("return document.body.scrollHeight")

        while True:
            driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
            time.sleep(SCROLL_PAUSE_TIME)
            new_height = driver.execute_script("return document.body.scrollHeight")
            if new_height == last_height:
                break
            last_height = new_height

        #soup = BeautifulSoup(driver.page_source, "html.parser")
        #
        #for c in soup("h2"):
        #    print(c.get_text())
        return driver.page_source

    finally:
        driver.quit()